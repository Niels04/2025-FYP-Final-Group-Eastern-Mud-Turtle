import numpy as np
import pandas as pd
from pathlib import Path

#for energy consumption measurement
from codecarbon import track_emissions

from sklearn.model_selection import GroupShuffleSplit, train_test_split
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, recall_score, roc_auc_score, roc_curve, confusion_matrix
from sklearn.utils import resample
from sklearn.inspection import DecisionBoundaryDisplay

import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

_FILE_DIR = Path(__file__).resolve().parent#obtain directory of this file
_PROJ_DIR = _FILE_DIR.parent#obtain main project directory
_RESULT_DIR = _PROJ_DIR / "result"#obtain results directory

class Performance:
    """Class to store performance metrics
    of a method on either training or validation/test data.
    """
    def __init__(self, ACCs: pd.DataFrame, RECs: pd.DataFrame, AUCs: pd.DataFrame, meanAcc:float, varAcc:float, meanRecall:float, varRecall:float, meanAUC:float, varAUC:float):
        #save means and variances (so they don't have to be computed each time we want to access them)
        self.meanAcc = meanAcc
        self.varAcc = varAcc
        self.meanRecall = meanRecall
        self.varRecall = varRecall
        self.meanAUC = meanAUC
        self.varAUC = varAUC
        #save actual performances for later boxplot generation
        self.ACCs = ACCs
        self.RECs = RECs
        self.AUCs = AUCs
    def __str__(self):
        return (
            f"\tMean Accuracy: {self.meanAcc:.4f}\n"
            f"\tVariance Accuracy: {self.varAcc:.4f}\n"
            f"\tMean Recall: {self.meanRecall:.4f}\n"
            f"\tVariance Recall: {self.varRecall:.4f}\n"
            f"\tMean AUC: {self.meanAUC:.4f}\n"
            f"\tVariance AUC: {self.varAUC:.4f}"
        )

class MethodPerformance:
    """Class to store performance metrics
    for a particular method on its training
    and validation/test data."""
    def __init__(self, trainPerf: Performance, valPerf: Performance):
        self.trainPerformance = trainPerf
        self.validationPerformance = valPerf

def makeGraphROC(name:str, yLabels: pd.DataFrame, yPredictedProbs: pd.DataFrame, dataType:str, combined = True) -> None:
        """Creates an ROC curve given a column of true yLabels and predicted yProbabilities.\n
        Saves the plotted curve as .png \"/result/\"with the provided method name as a suffix.\n
        :param name: name of the method for which the curve is generated
        :param yLabels: true yLabels
        :param yPredictedProbs: probabilities for label=1 generated by method
        :param dataType: specify if input data was training/validation/test data
        :param combined: specify whether the input data was from multiple shuffles or just a single run. Default: True
        :return None:"""

        fPosRate, tPosRate, _ = roc_curve(yLabels, yPredictedProbs)
        AUCscore = roc_auc_score(yLabels, yPredictedProbs)

        plt.figure(figsize=(8, 6))
        plt.plot(fPosRate, tPosRate, label=f"AUC = {AUCscore:.2f}", linewidth=2)
        plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")#display random guess middle line

        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        if combined:
            plt.title(f"Combined ROC Curve for method \"{name}\" on {dataType} data")
        else:
             plt.title(f"ROC Curve for method \"{name}\" on {dataType} data")
        plt.legend(loc="lower right")
        plt.grid(True)

        #save to png
        plt.savefig(str(_RESULT_DIR / f"roc_curve_{name}.png"), dpi=300, bbox_inches="tight")
        plt.close()#frees up the memory

def makeConfusionMatrix(name:str, yLabels: pd.DataFrame, yPredictions: pd.DataFrame, dataType:str, combined = 1) -> None:
        """Creates an Confusion Matrix plot given a column of true yLabels and yPredictions.\n
        Saves the plotted matrix as .png \"/result/\"with the provided method name as a suffix.\n
        :param name: name of the method for which the curve is generated
        :param yLabels: true yLabels
        :param yPredictions: 0 and 1 Melanoma predictions by the method
        :param dataType: specify if input data was training/validation/test data
        :param combined: specify how many runs/shuffles of the method were used to obtain the predictions (used for average calculation)
        :return None:"""

        confMat = confusion_matrix(yLabels, yPredictions)
        colors = [["lightgreen", "lightcoral"], ["lightcoral", "lightgreen"]]#colors to be used for the confusion matrix

        #plot the confusion matrix
        fig, axes = plt.subplots(figsize=(6,6))
        #set the labels for axes
        axes.set_xlabel("Predicted Labels")
        axes.set_ylabel("True Labels")
        axes.set_title(f"Confusion Matrix for \"{name}\" on {dataType} data", fontweight='bold', fontsize=16)

        classes = ["Non Melanoma", "Melanoma"]
        axes.set_xticks(np.arange(len(classes)) + 0.5)
        axes.set_yticks(np.arange(len(classes)) + 0.5)
        axes.set_xticklabels(classes)
        axes.set_yticklabels(classes)
        axes.tick_params(left=False, bottom=False)#don't display the little tick lines
        labels = [["True Negative", "False Positive"], ["False Negative", "True Positive"]]

        for i in range(2):
            for j in range(2):
                #draw the background rectangle
                rect = plt.Rectangle((j, i), 1, 1, facecolor=colors[i][j], edgecolor='black', linewidth=2)
                axes.add_patch(rect)

                #add text
                axes.text(j+0.5, i+0.5, f"{labels[i][j]}\n{confMat[i][j]/combined}",#divides the number from the confusion matrix by the number of runs/shuffles to normalize
                    ha="center", va="center", color="black", fontsize=10, fontweight="bold")
                
        #fix the axes so that 0,0 is at top-left and 2,2 is bottom-right
        axes.set_xlim(0, 2)
        axes.set_ylim(2, 0)
        axes.set_aspect("equal")

        plt.tight_layout()
        plt.savefig(str(_RESULT_DIR / f"confusion_matrix_{name}.png"), dpi=300, bbox_inches="tight")
        plt.close()

def printCrossValidationPerformance(name:str, data) -> None:
    """Given a name string and a data column, prints mean, standard deviation, 2.5th and 97.5th percentile.\n
    :param name: name of the data column
    :param data: data column as numpy array
    :return None:"""
    mean = np.mean(data)
    std = np.std(data)
    ci_lower = np.percentile(data, 2.5)
    ci_upper = np.percentile(data, 97.5)
    
    print(f"{name}:")
    print(f"\tMean: {mean:.4f}")
    print(f"\tStd Dev: {std:.4f}")
    print(f"\t95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]")

def finalCrossValidateClassifier(classifier, methodName:str, threshold:float, xTest:pd.DataFrame, yTest:pd.DataFrame, nStraps = 20) -> None:
    """Given a classifier, threshold and held-out test dataset, performs a cross validation via bootstrapping\n
    on the held-out test dataset to estimate performance metrics and variances for our method.\n
    :param classifier: classifier to cross-validate
    :param methodName: method name, e.g. \"base\" or \"extended\"
    :param threshold: decision threshold for the classifier
    :param xTest: test x data (features)
    :param yTest: y labels for test data, used to calculate performance metrics
    :return None:"""
    #store performances for individual straps
    AUCs = np.zeros(nStraps)#array to store AUC scores
    ACCs = np.zeros(nStraps)#array to store accuracy scores
    RECs = np.zeros(nStraps)#array to store recall scores
    #store all true labels and prediction probabilities for calculation of combined ROC curve at the end
    allYLabels = []
    allYPredictionProbs = []

    for i in range(nStraps):
        x, y = resample(xTest, yTest)#generate a bootstrap by resampling from the test data WITH REPLACEMENT
        yProbs = classifier.predict_proba(x)[:, 1]#get predicted probabilities from model
        #calculate performance metrics for current strap:
        #calculate AUC for current strap using the prediction probabilities
        try:
            AUCs[i] = roc_auc_score(y, yProbs)
        except:
            print("This error can occurr by chance if a random strap of the test data doesn't contain any Melanoma.\n Just try again :)")
        #compute accuracy and recall for current strap
        yPred = (yProbs >= threshold).astype(int)#turn prediction probabilities into binary classifications using the threshold
        ACCs[i] = accuracy_score(y, yPred)
        RECs[i] = recall_score(y, yPred)
        #save prediction probabilites and true labels for combined ROC curve & confusion matrix
        allYPredictionProbs.extend(yProbs)#save prediction probabilities for current strap
        allYLabels.extend(y)#save true labels for current strap

    #compute a combined ROC curve that takes into account predictions over all shuffles and save it to .png
    makeGraphROC(methodName, allYLabels, allYPredictionProbs, dataType="test")
        #NOTE: "Combined" means that the ROC curve is computed based on the predicted probabilities over all of the random bootstraps
    allYPredictionProbsNP = np.array(allYPredictionProbs)#convert to np array
    allYPred = (allYPredictionProbsNP > threshold)#turn probabilities into 0 and 1 classifications with threshold
    makeConfusionMatrix(methodName, allYLabels, allYPred, dataType="test", combined=nStraps)

    #finally print the performances
    print(f"Performance for cross validation for method \"{methodName}\"")
    printCrossValidationPerformance("AUC", AUCs)
    printCrossValidationPerformance("Accuracy", ACCs)
    printCrossValidationPerformance("Recall", RECs)


def runClassifier(classifier, methodName:str, threshold:float, xTrain:pd.DataFrame, yTrain:pd.DataFrame, xTest:pd.DataFrame, yTest:pd.DataFrame = None) -> pd.DataFrame:
    """Given training data and test data, perform a single run of the provided classifier\n
    and if true melanoma label is available for the test data, also compute some test statistics.\n
    :param classifier: classifier to run
    :param methodName: method name, e.g. \"base\" or \"extended\"
    :param threshold: decision threshold for the classifier
    :param xTrain: training x data(features)
    :param yTrain: y labels for training data
    :param xTest: test x data (features)
    :param yTest: (optional) y labels for test data, used to calculate performance metrics
    :return result: pandas dataframe with [\"img_id\", \"melanoma_prediction\"] + [\"true_melanoma_label\"] (if available)"""
    classifier.fit(xTrain, yTrain)#fit classifier with provided training data (training part of our dataset)

    testImgIDs = xTest["img_id"]#save img_id for result
    xTest = xTest.drop("img_id", axis=1)#drop img_id for prediction
    yProbs = classifier.predict_proba(xTest)[:, 1]#obtain column with melanoma prediction for test data

    #create result dataframe to store img_id and prediction (+ true label if available)
    result = pd.DataFrame({
    'img_id': testImgIDs,
    'melanoma_prediction': yProbs
    })

    if yTest is not None:
        #add true melanoma labels to result dataframe:
        result["true_melanoma_label"] = yTest

        #PERFORM CROSS VALIDATION
        finalCrossValidateClassifier(classifier, methodName, threshold, xTest, yTest)

    return result

def makeDecisionBoundary(feature1: str, feature2:str, classifier, name:str, xTrain:pd.DataFrame, yTrain:pd.DataFrame, threshold=0.5):
    """Given training data and the names of 2 feature columns, makes a scatterplot of the two features.\n
    The given classifier is fitted only on these two features and the resulting decision boundary is visualized on the scatterplot.\n
    :param feature1: name of the column for first feature.
    :param classifier: name of the column for second feature.
    :param classifier: classifier which should be fitted and for which the decision boundary should be visualized.
    :param name: name of the classifier to be displayed on the plot
    :param xTrain: independent variables of the training data
    :param yTrain: label column of the training data
    :param threshold: predicted probabilities above this threshold will be considered as Melanoma
    :return None:"""

    classifier.fit(xTrain[[feature1, feature2]], yTrain)#fit classifier with specified two features on training data

    xx, yy = np.meshgrid(np.linspace(0.0, 1.0, 500), np.linspace(0.0, 1.0, 500))
    grid = np.c_[xx.ravel(), yy.ravel()]
    # Get predicted probabilities for class 1
    probs = classifier.predict_proba(grid)[:, 1]
    yPred = (probs >= threshold).astype(int).reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    cmap_light = ListedColormap(['#FFBBBB', '#BBFFBB'])
    cmap_bold = ListedColormap(['#FF0000', '#00AA00'])

    plt.contourf(xx, yy, yPred, cmap=cmap_light, alpha=0.5)

    # Scatter original points
    scatter = plt.scatter(xTrain[feature1], xTrain[feature2], c=yTrain, cmap=cmap_bold, edgecolor='k', s=40)

    plt.xlabel(feature1)
    plt.ylabel(feature2)
    plt.title(f'Decision Boundary {name} (Threshold = {threshold})')
    plt.legend(*scatter.legend_elements(), title="Class")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(str(_RESULT_DIR / f"decision_boundary_{name}_{round(threshold, 3)}.png"), dpi=300)
    plt.close()


class Evaluator:
    def __init__(self):
        self.performances = {}
    
    #NOTE: This is for our own iterative process of finding the best classifier/trainSize/hyperparameters like tree depth, etc...
    #      For the final evaluation we could slightly re-write this method to measure prediction accuracy on actual test data
    #      (not validation data like its using right now)
    #      BUT I believe we shouldn't iterate & change our model after running it on the actual test data
    #      (would be Overfitting by Observer as described in the lecture)
    #      so I think it's super important that we keep "random_state=42" in the split_data the whole time

    def evalClassifier(self, classifier, name:str, xTrain:pd.DataFrame, yTrain:pd.DataFrame, patientGroups:pd.DataFrame, threshold:float, nShuffles = 20, validationSize=0.2, saveCurveROC = False, saveConfusionMatrix = False):
        """Given the classifier, computes AUC(accuracy) and
        recall (TP/(TP+FN)) over given number of grouped
        shuffles of the given training/validation data, grouped by the
        given column.\n
        Stores results in Evaluator class (can be printed with
        \"printPerformances()\").

        :param classifer: The untrained classifier to be evaluated
        :param name: Classifier name string
        :param xTrain: The x-columns of the training/working data
        :param yTrain: The y-column of the training/working data
        :param patientGroups: Column of training/working data to group by
        :param threshold: Decision Threshold from 0 to 1 (0.4 means that everything with probability of melanoma > 0.4 will be classified as melanoma)
        :param nShuffles: Number of different shuffled folds to perform, default=20
        :param validationSize: Proportion of training/working data to be used as validation Data, default=0.2
        :param saveCurveROC: If True, a combined ROC curve for this method(on the valdiation data) will be saved as .png in /result
        :param saveConfusionMatrix: If True, a confusion matrix for this method(on the validation data) will be saved as .png in /result
        :return None:"""
        ACCsTrain = np.zeros(nShuffles)#array to store accuracy scores for training data
        RECsTrain = np.zeros(nShuffles)#array to store recall scores for training data
        AUCsTrain = np.zeros(nShuffles)#array to store AUC scores (area under ROC curve) for training data
        ACCsVal = np.zeros(nShuffles)#array to store accuracy scores for validation data
        RECsVal = np.zeros(nShuffles)#array to store recall scores for validation data
        AUCsVal = np.zeros(nShuffles)#array to store AUC scores (area under ROC curve) for validation data
        
        #only for generating ROC curve over all shuffles (for validation data)
        allYLabels = []
        allYPredictionProbs = []

        #iterate through all splits, train the model and evaluate performance
        gss=GroupShuffleSplit(n_splits=nShuffles, test_size=validationSize)#not using random_state here for complete randomness
        for i, (trainIdx, valIdx) in enumerate(gss.split(xTrain, yTrain, patientGroups)):
            #FIT CLASSIFIER ON CURRENT SPLIT
            classifier.fit(xTrain.iloc[trainIdx], yTrain.iloc[trainIdx])

            #TEST CLASSIFIER ON CURRENT SPLIT

            #test on training data for current split---------------
            yProbs = classifier.predict_proba(xTrain.iloc[trainIdx])[:, 1]#predict melanoma probability for training data
            #calculate AUC for current shuffle using the prediction probabilities
            try:
                AUCsTrain[i] = roc_auc_score(yTrain.iloc[trainIdx], yProbs)
            except:
                print("This error can occurr by chance if a random shuffle of the test data doesn't contain any Melanoma.\n Just try again :)")
            #turn predicted probabilities into binary predictions using the given decision threshold
            yPred = (yProbs >= threshold).astype(int)
            #compute accuracy and recall for the given decision threshold
            ACCsTrain[i] = accuracy_score(yTrain.iloc[trainIdx], yPred)
            RECsTrain[i] = recall_score(yTrain.iloc[trainIdx], yPred)
            
            #test on validation data for current split--------------
            yProbs = classifier.predict_proba(xTrain.iloc[valIdx])[:, 1]#predict melanoma probability for evaluation data
            if saveCurveROC | saveConfusionMatrix:
                allYPredictionProbs.extend(yProbs)#save prediction probabilities for current shuffle for combined ROC curve computation/confusion matrix
                allYLabels.extend(yTrain.iloc[valIdx])#save true labels for current shuffle for combined ROC curve computation/confusion matrix
            #calculate AUC for current shuffle using the prediction probabilities
            try:
                AUCsVal[i] = roc_auc_score(yTrain.iloc[valIdx], yProbs)
            except:
                print("This error can occurr by chance if a random shuffle of the test data doesn't contain any Melanoma.\n Just try again :)")
            #turn predicted probabilities into binary predictions using the given decision threshold
            yPred = (yProbs >= threshold).astype(int)
            #compute accuracy and recall for the given decision threshold
            ACCsVal[i] = accuracy_score(yTrain.iloc[valIdx], yPred)
            RECsVal[i] = recall_score(yTrain.iloc[valIdx], yPred)

        trainPerformance = Performance(ACCsTrain, RECsTrain, AUCsTrain, np.mean(ACCsTrain), np.var(ACCsTrain), np.mean(RECsTrain), np.var(RECsTrain), np.mean(AUCsTrain), np.var(AUCsTrain))
        validationPerformance = Performance(ACCsVal, RECsVal, AUCsVal, np.mean(ACCsVal), np.var(ACCsVal), np.mean(RECsVal), np.var(RECsVal), np.mean(AUCsVal), np.var(AUCsVal))
        self.performances[name] = MethodPerformance(trainPerformance, validationPerformance)#store performance for current method in dict

        if saveCurveROC:#compute a combined ROC curve that takes into account predictions over all shuffles and save it to .png
            makeGraphROC(name, allYLabels, allYPredictionProbs, dataType="validation")
        #NOTE: "Combined" means that the ROC curve is computed based on the predicted probabilities over all of the random grouped
        #      shuffles FOR THE VALIDATION DATA
        if saveConfusionMatrix:#compute a combined confusion matrix (over all shuffles) and save as .png
            allYPredictionProbsNP = np.array(allYPredictionProbs)
            allYPred = (allYPredictionProbsNP > threshold)#turn probabilities into 0 and 1 classifications with threshold
            makeConfusionMatrix(name, allYLabels, allYPred, dataType="validation", combined=nShuffles)
    
    def printPerformances(self) -> None:
        """Print mean and variance of Accuracy, Recall and AUC for all methods to console.\n
        (for both training and validation runs)
        """
        for name, perf in self.performances.items():
            print(f"Performance for method \"{name}\"")
            print("On training data:")
            print(perf.trainPerformance)
            print("On validation data:")
            print(perf.validationPerformance)
            print("\n\n")

    def makeBoxplot(self, metric:str) -> None:
        """Generates a boxplot that compares the performances of all different
        methods stored inside evaluator class in a boxplot and saves it as .png in \"/result/\"\n
        :param metric: Performance metric to be compared, one of either: \"recall\", \"accuracy\" or \"AUC\"
        :return None:"""
        #obtain training and validation performances for all methods in a flattened list
        trainPerfs = []
        if metric == "recall":
            trainPerfs = [val for perf in self.performances.values() for val in (perf.trainPerformance.RECs, perf.validationPerformance.RECs)]
        elif metric == "accuracy":
            trainPerfs = [val for perf in self.performances.values() for val in (perf.trainPerformance.ACCs, perf.validationPerformance.ACCs)]
        elif metric == "AUC":
            trainPerfs = [val for perf in self.performances.values() for val in (perf.trainPerformance.AUCs, perf.validationPerformance.AUCs)]
        else:
            raise Exception("\"metric\" must be either \"recall\", \"accuracy\" or \"AUC\".")
        #obtain names of the corresponding methods
        methodNames = list(self.performances.keys())
        methodNames = [f"{name} ({suffix})" for name in methodNames for suffix in ("train", "val")]
        #add boxplot code here!
        #create boxplot
        fig, axes = plt.subplots(figsize=(10, 6))
        box = axes.boxplot(trainPerfs)
        axes.set_xticklabels(methodNames, rotation=45, ha='right')
        axes.set_title(f"Method {metric} Comparison")
        axes.set_ylabel(metric)
        axes.set_xlabel("Method")
        axes.grid(True, linestyle='--', alpha=0.7)

        #save plot
        plt.tight_layout()
        plt.savefig(str(_RESULT_DIR / f"classifier_performance_boxplot_{metric}.png"), dpi=300)
        plt.close()
#end of Evaluator class

def read(file):
    df=pd.read_csv(file)
    return df

def split_data(X:pd.DataFrame, y:pd.DataFrame, groupName:str):
    """Perform a grouped shuffle split on the whole data(always the same with random_state=42).\n
    Grouping is done on the column specified. The split is 80%/20%.\n
    :param X: x values from the dataset, including the grouping column.
    :param y: y values (labels) from the dataset.
    :param groupName: Name of the column from the x-values to group by.
    :return Split: xTrain, yTrain, xTest, yTest dataframes in this order."""
    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
    trainIdx, testIdx = next(gss.split(X, y, groups=X[groupName]))
    xTrain = X.iloc[trainIdx]
    yTrain = y.iloc[trainIdx]
    xTest = X.iloc[testIdx]
    yTest = y.iloc[testIdx]
    return xTrain, yTrain, xTest, yTest

#NOTE: We could try other stuff here like different parameters for K in KNN
#      or different max_depth for Tree or Forest and compare the performances.
#      Also we could try shrinking the training data and seeing if we can still
#      get acceptable performance with notably less training data
#      (could plot trainSize vs. performance on training data & test data)
#      For the report we could use the output to conduct a statistical test
#      whether one method is better than the other at some confidence level.

@track_emissions(country_iso_code="DNK")
def main():
    featureFile = str(_PROJ_DIR / "data/features.csv")
    df=read(featureFile)
    df=df.drop(axis=1,labels=["img_id", "patient_id", "lesion_id"])#drop unnecessary columns
    y = df['true_melanoma_label']#obtain melanoma binary label-column as y-data
    X = df.drop(['true_melanoma_label'], axis=1)#drop melanoma binary label to only leave x-data (also leaves pat_les_id for later grouping)
    xTrain, yTrain, xTest, yTest = split_data(X, y, "pat_les_ID")#split grouping by lesion
    patientGroup=xTrain["pat_les_ID"]#obtain grouping column for training/working data (grouping by patient_id) (NOT over the whole dataset but only over the training data)
    xTrain = xTrain.drop(["pat_les_ID"], axis=1)#get rid of pat_les_id in training/working X-data
    xTest = xTest.drop(["pat_les_ID"], axis=1)#get rid of pat_les_id in test X-data

    #test different classifiers on the training/working data:
    clf1 = RandomForestClassifier(class_weight="balanced",max_depth=5) #for the base max_depth 2 and 5 for the extended
    clf2 = DecisionTreeClassifier(class_weight="balanced",max_depth=5) #for the base max_depth 2 and 5 for the extended
    #clf3 = KNeighborsClassifier(weights='distance',n_neighbors=1,algorithm='brute')
    clf4 = LogisticRegression(class_weight="balanced",max_iter=100000)

# If we use clf1 maxdepth 1 and clf2 maxdepth 5 accuracy 80°% and recall 50%
# If we set maxdepth to be more than 10 it just overfit
# if we set maxdepth to be more than 5 we get high accuracy however the recall is between 20-40%
# if we set maxdepth to be 3 clf2: accuracy gets not that consistant (between 55-78% avg: 65), however the recall gets 60%
# if we set maxdepth to be 3 clf1: accuracy 85% and the recall is consistent on avg 40%
# if we set maxdepth to be 1 clf1: accuracy 80% and the recall is not that consistant on avg 50%

    voting_clf = VotingClassifier(estimators=[
        ('rf', clf1), 
        ('dt', clf2), 
        ('lr',clf4)
        ], voting='soft')#voting="hard" produces an error in AUC calculation, so don't use it for now

    eval = Evaluator()
    eval.evalClassifier(clf1, "RandomForest", xTrain, yTrain, patientGroup, threshold=0.3) # Threshold on 0.3 and maxdepth 5 improves the everything
    eval.evalClassifier(clf2, "DecisionTree", xTrain, yTrain, patientGroup, threshold=0.5) # It doesn't improve it. It just makes it overfit or underfit
    #eval.evalClassifier(clf3, "KNN", xTrain, yTrain, patientGroup, threshold=0.5) #NO use of modifying the threshold  
    eval.evalClassifier(voting_clf, "Voting", xTrain, yTrain, patientGroup, threshold=0.4)
    eval.evalClassifier(clf4, "Logistic Regression", xTrain, yTrain, patientGroup, threshold=0.5, saveConfusionMatrix=True)
        
    xTrainStripped = xTrain[["fA_score", "fC_score", "fBV_score", "fS_score"]]#only use promising features
    eval.evalClassifier(clf4, "LogisticRegression_Stripped", xTrainStripped, yTrain, patientGroup, threshold=0.5, saveCurveROC=True, saveConfusionMatrix=True)
    makeDecisionBoundary("fBV_score", "fA_score", clf4, "Logistic Regression", xTrain, yTrain, threshold=0.5)

    #eval.printPerformances()
    eval.makeBoxplot("AUC")
    eval.makeBoxplot("recall")
    eval.makeBoxplot("accuracy")


if __name__ == "__main__":
    main()

#Gergely's note
''' Randomforest:There is no point of inrceasing the max depth, the opposite, the lower the number the better.
            Currently the best is depth of 2
    DecisionTree: Similar to the Randomforest
    KNN: Because of the small amount of MEL pictures, using KNN is a catastrophy. Only produces minimal 'good' results if k=1 
            (Most of the data point are non-MEL that means if we look at the neighbor there is a higher chance of finding non-MEL.)
    Logistic:Increasing the iteration, increases both the accuracy and recall on avg!
            (There is no point increasing it over 100_000)
    Features:
    A: More stability
    B: More variaty
    C: chaotic, not consistent, huge variaty
    AB:  We get an avg: recall 0.5 and accuarcy 0.6
    BC:  We get an avg: recall 0.3 and accuarcy 0.6
    AC:  We get an avg: recall 0.6 and accuarcy 0.6
    ABC: We get an avg: recall 0.6 and accuarcy 0.6
    [based on the voting]
    
    In overall, we can try to modify 3 clf to get something which is relatively accurate OR we just use the Logistic regression, it's consistent and accurate WITHOUT modifing it  
    
    
    '''